var documenterSearchIndex = {"docs":
[{"location":"#MultivariateAnomalies.jl-1","page":"Home","title":"MultivariateAnomalies.jl","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"A julia package for detecting multivariate anomalies.","category":"page"},{"location":"#","page":"Home","title":"Home","text":"Keywords: Novelty detection, Anomaly Detection, Outlier Detection, Statistical Process Control, Process Monitoring","category":"page"},{"location":"#","page":"Home","title":"Home","text":"Please cite this package as: Flach, M., Gans, F., Brenning, A., Denzler, J., Reichstein, M., Rodner, E., Bathiany, S., Bodesheim, P., Guanche, Y., Sippel, S., and Mahecha, M. D. (2017): Multivariate anomaly detection for Earth observations: a comparison of algorithms and feature extraction techniques, Earth Syst. Dynam., 8, 677-696, doi:10.5194/esd-8-677-2017.","category":"page"},{"location":"#Requirements-1","page":"Home","title":"Requirements","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"Julia >= 0.7\nJulia packages Distances, Combinatorics, LinearAlgebra, and LIBSVM.","category":"page"},{"location":"#Installation-1","page":"Home","title":"Installation","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"add the package: ]add MultivariateAnomalies","category":"page"},{"location":"#Package-Features-1","page":"Home","title":"Package Features","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"Detect anomalies in your data with easy to use high level functions or individual anomaly detection algorithms\nFeature Extraction: Preprocess your data by extracting relevant features\nSimilarities and Dissimilarities: Compute distance matrices, kernel matrices and k-nearest neighbor objects.\nPostprocessing: Postprocess your anomaly scores, by computing their quantiles or combinations of several algorithms (ensembles).\nAUC: Compute the area under the curve as external evaluation metric of your scores.\nOnline Algorithms: Algorithms tuned for little memory allocation.","category":"page"},{"location":"#Using-the-Package-1","page":"Home","title":"Using the Package","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"For a quick start it might be useful to start with the high level functions for detecting anomalies. They can be used in highly automized way. ","category":"page"},{"location":"#Input-Data-1","page":"Home","title":"Input Data","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"MultivariateAnomalies.jl assumes that observations/samples/time steps are stored along the first dimension of the data array (rows of a matrix) with the number of observations T = size(data, 1). Variables/attributes are stored along the last dimension N of the data array (along the columns of a matrix) with the number of variables VAR = size(data, N). The implemented anomaly detection algorithms return anomaly scores indicating which observation(s) of the data are anomalous.","category":"page"},{"location":"#Authors-1","page":"Home","title":"Authors","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"<img align=\"right\" src=\"img/MPG_Minerva.png\" alt=\"Minerva\" width=\"75\"/>  The package was implemented by Milan Flach and Fabian Gans, Max Planck Institute for Biogeochemistry, Department Biogeochemical Integration, Research group for Empirical Inference of the Earth System, Jena. ","category":"page"},{"location":"#Index-1","page":"Home","title":"Index","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"Pages = [\"man/Preprocessing.md\", \"man/DetectionAlgorithms.md\", \"man/Postprocessing.md\", \"man/AUC.md\", \"man/DistancesDensity.md\", \"man/OnlineAlgorithms.md\"]","category":"page"},{"location":"man/HighLevelFunctions/#High-Level-Anomaly-Detection-Algorithms-1","page":"High Level Functions","title":"High Level Anomaly Detection Algorithms","text":"","category":"section"},{"location":"man/HighLevelFunctions/#","page":"High Level Functions","title":"High Level Functions","text":"We provide high-level convenience functions for detecting the anomalies. Namely the pair of ","category":"page"},{"location":"man/HighLevelFunctions/#","page":"High Level Functions","title":"High Level Functions","text":"P = getParameters(algorithms, training_data)  and detectAnomalies(testing_data, P)","category":"page"},{"location":"man/HighLevelFunctions/#","page":"High Level Functions","title":"High Level Functions","text":"sets standard choices of the Parameters P and hands the parameters as well as the algorithms choice over to detect the anomalies. ","category":"page"},{"location":"man/HighLevelFunctions/#","page":"High Level Functions","title":"High Level Functions","text":"Currently supported algorithms include Kernel Density Estimation (algorithms = [\"KDE\"]), Recurrences (\"REC\"), k-Nearest Neighbors algorithms (\"KNN-Gamma\", \"KNN-Delta\"), Hotelling's T^2 (\"T2\"), Support Vector Data Description (\"SVDD\") and Kernel Null Foley Summon Transform (\"KNFST\"). With getParameters() it is also possible to compute output scores of multiple algorithms at once (algorihtms = [\"KDE\", \"T2\"]), quantiles of the output anomaly scores (quantiles = true) and ensembles of the selected algorithms (e.g. ensemble_method = \"mean\"). ","category":"page"},{"location":"man/HighLevelFunctions/#Functions-1","page":"High Level Functions","title":"Functions","text":"","category":"section"},{"location":"man/HighLevelFunctions/#","page":"High Level Functions","title":"High Level Functions","text":"getParameters\ndetectAnomalies\ndetectAnomalies!\ninit_detectAnomalies","category":"page"},{"location":"man/HighLevelFunctions/#MultivariateAnomalies.getParameters","page":"High Level Functions","title":"MultivariateAnomalies.getParameters","text":"getParameters(algorithms::Array{String,1} = [\"REC\", \"KDE\"], training_data::AbstractArray{tp, 2} = [NaN NaN])\n\nreturn an object of type PARAMS, given the algorithms and some training_data as a matrix.\n\nArguments\n\nalgorithms: Subset of [\"REC\", \"KDE\", \"KNN_Gamma\", \"KNN_Delta\", \"SVDD\", \"KNFST\", \"T2\"]\ntraining_data: data for training the algorithms / for getting the Parameters.\ndist::String = \"Euclidean\"\nsigma_quantile::Float64 = 0.5 (median): quantile of the distance matrix, used to compute the weighting parameter for the kernel matrix (algorithms = [\"SVDD\", \"KNFST\", \"KDE\"])\nvarepsilon_quantile = sigma_quantile by default: quantile of the distance matrix to compute the radius of the hyperball in which the number of reccurences is counted (algorihtms = [\"REC\"])\nk_perc::Float64 = 0.05: percentage of the first dimension of training_data to estimmate the number of nearest neighbors (algorithms = [\"KNN-Gamma\", \"KNN_Delta\"])\nnu::Float64 = 0.2: use the maximal percentage of outliers for algorithms = [\"SVDD\"]\ntemp_excl::Int64 = 0. Exclude temporal adjacent points from beeing count as recurrences of k-nearest neighbors algorithms = [\"REC\", \"KNN-Gamma\", \"KNN_Delta\"]\nensemble_method = \"None\": compute an ensemble of the used algorithms. Possible choices (given in compute_ensemble()) are \"mean\", \"median\", \"max\" and \"min\".\nquantiles = false: convert the output scores of the algorithms into quantiles.\n\nExamples\n\njulia> using MultivariateAnomalies\njulia> training_data = randn(100, 2); testing_data = randn(100, 2);\njulia> P = getParameters([\"REC\", \"KDE\", \"SVDD\"], training_data, quantiles = false);\njulia> detectAnomalies(testing_data, P)\n\n\n\n\n\n","category":"function"},{"location":"man/HighLevelFunctions/#MultivariateAnomalies.detectAnomalies","page":"High Level Functions","title":"MultivariateAnomalies.detectAnomalies","text":"detectAnomalies(data::AbstractArray{tp, N}, P::PARAMS) where {tp, N}\ndetectAnomalies(data::AbstractArray{tp, N}, algorithms::Array{String,1} = [\"REC\", \"KDE\"]; mean = 0) where {tp, N}\n\ndetect anomalies, given some Parameter object P of type PARAMS. Train the Parameters P with getParameters() beforehand on some training data. See getParameters(). Without training P beforehand, it is also possible to use detectAnomalies(data, algorithms) given some algorithms (except SVDD, KNFST). Some default parameters are used in this case to initialize P internally.\n\nExamples\n\njulia> training_data = randn(100, 2); testing_data = randn(100, 2);\njulia> # compute the anoamly scores of the algorithms \"REC\", \"KDE\", \"T2\" and \"KNN_Gamma\", their quantiles and return their ensemble scores\njulia> P = getParameters([\"REC\", \"KDE\", \"T2\", \"KNN_Gamma\"], training_data, quantiles = true, ensemble_method = \"mean\");\njulia> detectAnomalies(testing_data, P)\n\n\n\n\n\n","category":"function"},{"location":"man/HighLevelFunctions/#MultivariateAnomalies.detectAnomalies!","page":"High Level Functions","title":"MultivariateAnomalies.detectAnomalies!","text":"detectAnomalies!{tp, N}(data::AbstractArray{tp, N}, P::PARAMS)\n\nmutating version of detectAnomalies(). Directly writes the output into P.\n\n\n\n\n\n","category":"function"},{"location":"man/HighLevelFunctions/#MultivariateAnomalies.init_detectAnomalies","page":"High Level Functions","title":"MultivariateAnomalies.init_detectAnomalies","text":"init_detectAnomalies{tp, N}(data::AbstractArray{tp, N}, P::PARAMS)\n\ninitialize empty arrays in P for detecting the anomalies.\n\n\n\n\n\n","category":"function"},{"location":"man/HighLevelFunctions/#Index-1","page":"High Level Functions","title":"Index","text":"","category":"section"},{"location":"man/HighLevelFunctions/#","page":"High Level Functions","title":"High Level Functions","text":"Pages = [\"HighLevelFunctions.md\"]","category":"page"},{"location":"man/DetectionAlgorithms/#Anomaly-Detection-Algorithms-1","page":"Anomaly Detection Algorithms","title":"Anomaly Detection Algorithms","text":"","category":"section"},{"location":"man/DetectionAlgorithms/#","page":"Anomaly Detection Algorithms","title":"Anomaly Detection Algorithms","text":"Most of the anomaly detection algorithms below work on a distance/similarity matrix D or a kernel/dissimilarity matrix K. They can be comuted using the functions provided here.","category":"page"},{"location":"man/DetectionAlgorithms/#","page":"Anomaly Detection Algorithms","title":"Anomaly Detection Algorithms","text":"Currently supported algorithms include","category":"page"},{"location":"man/DetectionAlgorithms/#","page":"Anomaly Detection Algorithms","title":"Anomaly Detection Algorithms","text":"Recurrences (REC)\nKernel Density Estimation (KDE)\nHotelling's T^2 (Mahalanobis distance) (T2)\ntwo k-Nearest Neighbor approaches (KNN-Gamma, KNN-Delta)  \nUnivariate Approach (UNIV)\nSupport Vector Data Description (SVDD)\nKernel Null Foley Summon Transform (KNFST)","category":"page"},{"location":"man/DetectionAlgorithms/#Functions-1","page":"Anomaly Detection Algorithms","title":"Functions","text":"","category":"section"},{"location":"man/DetectionAlgorithms/#Recurrences-1","page":"Anomaly Detection Algorithms","title":"Recurrences","text":"","category":"section"},{"location":"man/DetectionAlgorithms/#","page":"Anomaly Detection Algorithms","title":"Anomaly Detection Algorithms","text":"REC\nREC!\ninit_REC","category":"page"},{"location":"man/DetectionAlgorithms/#MultivariateAnomalies.REC","page":"Anomaly Detection Algorithms","title":"MultivariateAnomalies.REC","text":"REC(D::AbstractArray, rec_threshold::Float64, temp_excl::Int = 0)\n\nCount the number of observations (recurrences) which fall into a radius rec_threshold of a distance matrix D. Exclude steps which are closer than temp_excl to be count as recurrences (default: temp_excl = 5)\n\nMarwan, N., Carmen Romano, M., Thiel, M., & Kurths, J. (2007). Recurrence plots for the analysis of complex systems. Physics Reports, 438(5-6), 237–329. http://doi.org/10.1016/j.physrep.2006.11.001\n\n\n\n\n\n","category":"function"},{"location":"man/DetectionAlgorithms/#MultivariateAnomalies.REC!","page":"Anomaly Detection Algorithms","title":"MultivariateAnomalies.REC!","text":"REC!(rec_out::AbstractArray, D::AbstractArray, rec_threshold::Float64, temp_excl::Int = 0)\n\nMemory efficient version of REC() for use within a loop. rec_out is preallocated output, should be initialised with init_REC().\n\n\n\n\n\n","category":"function"},{"location":"man/DetectionAlgorithms/#MultivariateAnomalies.init_REC","page":"Anomaly Detection Algorithms","title":"MultivariateAnomalies.init_REC","text":"init_REC(D::Array{Float64, 2})\ninit_REC(T::Int)\n\nget object for memory efficient REC!() versions. Input can be a distance matrix D or the number of timesteps (observations) T.\n\n\n\n\n\n","category":"function"},{"location":"man/DetectionAlgorithms/#Kernel-Density-Estimation-1","page":"Anomaly Detection Algorithms","title":"Kernel Density Estimation","text":"","category":"section"},{"location":"man/DetectionAlgorithms/#","page":"Anomaly Detection Algorithms","title":"Anomaly Detection Algorithms","text":"KDE\nKDE!\ninit_KDE","category":"page"},{"location":"man/DetectionAlgorithms/#MultivariateAnomalies.KDE","page":"Anomaly Detection Algorithms","title":"MultivariateAnomalies.KDE","text":"KDE(K)\n\nCompute a Kernel Density Estimation (the Parzen sum), given a Kernel matrix K.\n\nParzen, E. (1962). On Estimation of a Probability Density Function and Mode. The Annals of Mathematical Statistics, 33, 1–1065–1076.\n\n\n\n\n\n","category":"function"},{"location":"man/DetectionAlgorithms/#MultivariateAnomalies.KDE!","page":"Anomaly Detection Algorithms","title":"MultivariateAnomalies.KDE!","text":"KDE!(KDE_out, K)\n\nMemory efficient version of KDE(). Additionally uses preallocated KDE_out object for writing the results. Initialize KDE_out with init_KDE().\n\n\n\n\n\n","category":"function"},{"location":"man/DetectionAlgorithms/#MultivariateAnomalies.init_KDE","page":"Anomaly Detection Algorithms","title":"MultivariateAnomalies.init_KDE","text":"init_KDE(K::Array{Float64, 2})\ninit_KDE(T::Int)\n\nReturns KDE_out object for usage in KDE!(). Use either a Kernel matrix K or the number of time steps/observations T as argument.\n\n\n\n\n\n","category":"function"},{"location":"man/DetectionAlgorithms/#Hotelling's-Tsup2/sup-1","page":"Anomaly Detection Algorithms","title":"Hotelling's T<sup>2</sup>","text":"","category":"section"},{"location":"man/DetectionAlgorithms/#","page":"Anomaly Detection Algorithms","title":"Anomaly Detection Algorithms","text":"T2\nT2!\ninit_T2","category":"page"},{"location":"man/DetectionAlgorithms/#MultivariateAnomalies.T2","page":"Anomaly Detection Algorithms","title":"MultivariateAnomalies.T2","text":"T2{tp}(data::AbstractArray{tp,2}, Q::AbstractArray[, mv])\n\nCompute Hotelling's T2 control chart (the squared Mahalanobis distance to the data's mean vector (mv), given the covariance matrix Q). Input data is a two dimensional data matrix (observations * variables).\n\nLowry, C. A., & Woodall, W. H. (1992). A Multivariate Exponentially Weighted Moving Average Control Chart. Technometrics, 34, 46–53.\n\n\n\n\n\n","category":"function"},{"location":"man/DetectionAlgorithms/#MultivariateAnomalies.T2!","page":"Anomaly Detection Algorithms","title":"MultivariateAnomalies.T2!","text":"T2!(t2_out, data, Q[, mv])\n\nMemory efficient version of T2(), for usage within a loop etc. Initialize the t2_out object with init_T2(). t2_out[1] contains the squred Mahalanobis distance after computation.\n\n\n\n\n\n","category":"function"},{"location":"man/DetectionAlgorithms/#MultivariateAnomalies.init_T2","page":"Anomaly Detection Algorithms","title":"MultivariateAnomalies.init_T2","text":"init_T2(VAR::Int, T::Int)\ninit_T2{tp}(data::AbstractArray{tp,2})\n\ninitialize t2_out object for T2! either with number of variables VAR and observations/time steps T or with a two dimensional data matrix (time * variables)\n\n\n\n\n\n","category":"function"},{"location":"man/DetectionAlgorithms/#k-Nearest-Neighbors-1","page":"Anomaly Detection Algorithms","title":"k-Nearest Neighbors","text":"","category":"section"},{"location":"man/DetectionAlgorithms/#","page":"Anomaly Detection Algorithms","title":"Anomaly Detection Algorithms","text":"KNN_Gamma\nKNN_Gamma!\ninit_KNN_Gamma\nKNN_Delta\nKNN_Delta!\ninit_KNN_Delta","category":"page"},{"location":"man/DetectionAlgorithms/#MultivariateAnomalies.KNN_Gamma","page":"Anomaly Detection Algorithms","title":"MultivariateAnomalies.KNN_Gamma","text":"KNN_Gamma(knn_dists_out)\n\nThis function computes the mean distance of the K nearest neighbors given a knn_dists_out object from knn_dists() as input argument.\n\nHarmeling, S., Dornhege, G., Tax, D., Meinecke, F., & Müller, K.-R. (2006). From outliers to prototypes: Ordering data. Neurocomputing, 69(13-15), 1608–1618. http://doi.org/10.1016/j.neucom.2005.05.015\n\n\n\n\n\n","category":"function"},{"location":"man/DetectionAlgorithms/#MultivariateAnomalies.KNN_Gamma!","page":"Anomaly Detection Algorithms","title":"MultivariateAnomalies.KNN_Gamma!","text":"KNN_Gamma!(KNN_Gamma_out, knn_dists_out)\n\nMemory efficient version of KNN_Gamma, to be used in a loop. Initialize KNN_Gamma_out with init_KNN_Gamma().\n\n\n\n\n\n","category":"function"},{"location":"man/DetectionAlgorithms/#MultivariateAnomalies.init_KNN_Gamma","page":"Anomaly Detection Algorithms","title":"MultivariateAnomalies.init_KNN_Gamma","text":"init_KNN_Gamma(T::Int)\ninit_KNN_Gamma(knn_dists_out)\n\ninitialize a KNN_Gamma_out object for KNN_Gamma! either with T, the number of observations/time steps or with a knn_dists_out object.\n\n\n\n\n\n","category":"function"},{"location":"man/DetectionAlgorithms/#MultivariateAnomalies.KNN_Delta","page":"Anomaly Detection Algorithms","title":"MultivariateAnomalies.KNN_Delta","text":"KNN_Delta(knn_dists_out, data)\n\nCompute Delta as vector difference of the k-nearest neighbors. Arguments are a knn_dists() object (knn_dists_out) and a data matrix (observations * variables)\n\nHarmeling, S., Dornhege, G., Tax, D., Meinecke, F., & Müller, K.-R. (2006). From outliers to prototypes: Ordering data. Neurocomputing, 69(13-15), 1608–1618. http://doi.org/10.1016/j.neucom.2005.05.015\n\n\n\n\n\n","category":"function"},{"location":"man/DetectionAlgorithms/#MultivariateAnomalies.KNN_Delta!","page":"Anomaly Detection Algorithms","title":"MultivariateAnomalies.KNN_Delta!","text":"KNN_Delta!(KNN_Delta_out, knn_dists_out, data)\n\nMemory Efficient Version of KNN_Delta(). KNN_Delta_out[1] is the vector difference of the k-nearest neighbors.\n\n\n\n\n\n","category":"function"},{"location":"man/DetectionAlgorithms/#MultivariateAnomalies.init_KNN_Delta","page":"Anomaly Detection Algorithms","title":"MultivariateAnomalies.init_KNN_Delta","text":"init_KNN_Delta(T, VAR, k)\n\nreturn a KNN_Delta_out object to be used for KNN_Delta!. Input: time steps/observations T, variables VAR, number of K nearest neighbors k.\n\n\n\n\n\n","category":"function"},{"location":"man/DetectionAlgorithms/#Univariate-Approach-1","page":"Anomaly Detection Algorithms","title":"Univariate Approach","text":"","category":"section"},{"location":"man/DetectionAlgorithms/#","page":"Anomaly Detection Algorithms","title":"Anomaly Detection Algorithms","text":"UNIV\nUNIV!\ninit_UNIV","category":"page"},{"location":"man/DetectionAlgorithms/#MultivariateAnomalies.UNIV","page":"Anomaly Detection Algorithms","title":"MultivariateAnomalies.UNIV","text":"UNIV(data)\n\norder the values in each varaible and return their maximum, i.e. any of the variables in data (observations * variables) is above a given quantile, the highest quantile will be returned.\n\n\n\n\n\n","category":"function"},{"location":"man/DetectionAlgorithms/#MultivariateAnomalies.UNIV!","page":"Anomaly Detection Algorithms","title":"MultivariateAnomalies.UNIV!","text":"UNIV!(univ_out, data)\n\nMemory efficient version of UNIV(), input an univ_out object from init_UNIV() and some data matrix observations * variables\n\n\n\n\n\n","category":"function"},{"location":"man/DetectionAlgorithms/#MultivariateAnomalies.init_UNIV","page":"Anomaly Detection Algorithms","title":"MultivariateAnomalies.init_UNIV","text":"init_UNIV(T::Int, VAR::Int)\ninit_UNIV{tp}(data::AbstractArray{tp, 2})\n\ninitialize a univ_out object to be used in UNIV!() either with number of time steps/observations T and variables VAR or with a data matrix observations * variables.\n\n\n\n\n\n","category":"function"},{"location":"man/DetectionAlgorithms/#Support-Vector-Data-Description-1","page":"Anomaly Detection Algorithms","title":"Support Vector Data Description","text":"","category":"section"},{"location":"man/DetectionAlgorithms/#","page":"Anomaly Detection Algorithms","title":"Anomaly Detection Algorithms","text":"SVDD_train\nSVDD_predict","category":"page"},{"location":"man/DetectionAlgorithms/#MultivariateAnomalies.SVDD_train","page":"Anomaly Detection Algorithms","title":"MultivariateAnomalies.SVDD_train","text":"SVDD_train(K, nu)\n\ntrain a one class support vecort machine model (i.e. support vector data description), given a kernel matrix K and and the highest possible percentage of outliers nu. Returns the model object (svdd_model). Requires LIBSVM.\n\nTax, D. M. J., & Duin, R. P. W. (1999). Support vector domain description. Pattern Recognition Letters, 20, 1191–1199. Schölkopf, B., Williamson, R. C., & Bartlett, P. L. (2000). New Support Vector Algorithms. Neural Computation, 12, 1207–1245.\n\n\n\n\n\n","category":"function"},{"location":"man/DetectionAlgorithms/#MultivariateAnomalies.SVDD_predict","page":"Anomaly Detection Algorithms","title":"MultivariateAnomalies.SVDD_predict","text":"SVDD_predict(svdd_model, K)\n\npredict the outlierness of an object given the testing Kernel matrix K and the svdd_model from SVDD_train(). Requires LIBSVM.\n\nTax, D. M. J., & Duin, R. P. W. (1999). Support vector domain description. Pattern Recognition Letters, 20, 1191–1199. Schölkopf, B., Williamson, R. C., & Bartlett, P. L. (2000). New Support Vector Algorithms. Neural Computation, 12, 1207–1245.\n\n\n\n\n\n","category":"function"},{"location":"man/DetectionAlgorithms/#Kernel-Null-Foley-Summon-Transform-1","page":"Anomaly Detection Algorithms","title":"Kernel Null Foley Summon Transform","text":"","category":"section"},{"location":"man/DetectionAlgorithms/#","page":"Anomaly Detection Algorithms","title":"Anomaly Detection Algorithms","text":"KNFST_train\nKNFST_predict\nKNFST_predict!\ninit_KNFST  ","category":"page"},{"location":"man/DetectionAlgorithms/#MultivariateAnomalies.KNFST_train","page":"Anomaly Detection Algorithms","title":"MultivariateAnomalies.KNFST_train","text":"KNFST_train(K)\n\ntrain a one class novelty KNFST model on a Kernel matrix K according to Paul Bodesheim and Alexander Freytag and Erik Rodner and Michael Kemmler and Joachim Denzler: \"Kernel Null Space Methods for Novelty Detection\". Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2013.\n\nOutput\n\n(proj, targetValue) proj \t– projection vector for data points (project x via kx*proj, where kx is row vector containing kernel values of x and training data) targetValue – value of all training samples in the null space\n\n\n\n\n\n","category":"function"},{"location":"man/DetectionAlgorithms/#MultivariateAnomalies.KNFST_predict","page":"Anomaly Detection Algorithms","title":"MultivariateAnomalies.KNFST_predict","text":"KNFST_predict(model, K)\n\npredict the outlierness of some data (represented by the kernel matrix K), given some KNFST model from KNFST_train(K). Compute Kwith kernel_matrix().\n\nPaul Bodesheim and Alexander Freytag and Erik Rodner and Michael Kemmler and Joachim Denzler: \"Kernel Null Space Methods for Novelty Detection\". Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2013.\n\n\n\n\n\n","category":"function"},{"location":"man/DetectionAlgorithms/#MultivariateAnomalies.KNFST_predict!","page":"Anomaly Detection Algorithms","title":"MultivariateAnomalies.KNFST_predict!","text":"KNFST_predict!(KNFST_out, KNFST_mod, K)\n\npredict the outlierness of some data (represented by the kernel matrix K), given a KNFST_out object (init_KNFST()), some KNFST model (KNFST_mod = KNFST_train(K)) and the testing kernel matrix K.\n\nPaul Bodesheim and Alexander Freytag and Erik Rodner and Michael Kemmler and Joachim Denzler: \"Kernel Null Space Methods for Novelty Detection\". Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2013.\n\n\n\n\n\n","category":"function"},{"location":"man/DetectionAlgorithms/#MultivariateAnomalies.init_KNFST","page":"Anomaly Detection Algorithms","title":"MultivariateAnomalies.init_KNFST","text":"init_KNFST(T, KNFST_mod)\n\ninitialize a KNFST_outobject for the use with KNFST_predict!, given T, the number of observations and the model output KNFST_train(K).\n\n\n\n\n\n","category":"function"},{"location":"man/DetectionAlgorithms/#Distance-to-some-Centers-1","page":"Anomaly Detection Algorithms","title":"Distance to some Centers","text":"","category":"section"},{"location":"man/DetectionAlgorithms/#","page":"Anomaly Detection Algorithms","title":"Anomaly Detection Algorithms","text":"Dist2Centers","category":"page"},{"location":"man/DetectionAlgorithms/#MultivariateAnomalies.Dist2Centers","page":"Anomaly Detection Algorithms","title":"MultivariateAnomalies.Dist2Centers","text":"Dist2Centers(centers::AbstractArray{tp, 2}) where {tp}\n\nCompute the distance to the nearest centers of i.e. a K-means clustering output. Large Distances to the nearest center are anomalies. data: Observations * Variables.\n\nExample\n\n(proj, targetValue)\n\n\n\n\n\n","category":"function"},{"location":"man/DetectionAlgorithms/#Index-1","page":"Anomaly Detection Algorithms","title":"Index","text":"","category":"section"},{"location":"man/DetectionAlgorithms/#","page":"Anomaly Detection Algorithms","title":"Anomaly Detection Algorithms","text":"Pages = [\"DetectionAlgorithms\"]","category":"page"},{"location":"man/DistancesDensity/#Distance,-Kernel-Matrices-and-k-Nearest-Neighbours-1","page":"Distance and Densities","title":"Distance, Kernel Matrices and k-Nearest Neighbours","text":"","category":"section"},{"location":"man/DistancesDensity/#","page":"Distance and Densities","title":"Distance and Densities","text":"Compute distance matrices (similarity matrices) and convert them into kernel matrices or k-nearest neighbor objects.","category":"page"},{"location":"man/DistancesDensity/#Distance/Similarity-Matrices-1","page":"Distance and Densities","title":"Distance/Similarity Matrices","text":"","category":"section"},{"location":"man/DistancesDensity/#","page":"Distance and Densities","title":"Distance and Densities","text":"A distance matrix D consists of pairwise distances d()computed with some metrix (e.g. Euclidean):","category":"page"},{"location":"man/DistancesDensity/#","page":"Distance and Densities","title":"Distance and Densities","text":"D = d(X_t_i X_t_j)","category":"page"},{"location":"man/DistancesDensity/#","page":"Distance and Densities","title":"Distance and Densities","text":"i.e. the distance between vector X of observation t_i and t_j for all observations t_it_j = 1 ldots T.","category":"page"},{"location":"man/DistancesDensity/#Functions-1","page":"Distance and Densities","title":"Functions","text":"","category":"section"},{"location":"man/DistancesDensity/#","page":"Distance and Densities","title":"Distance and Densities","text":"dist_matrix\ndist_matrix!\ninit_dist_matrix","category":"page"},{"location":"man/DistancesDensity/#MultivariateAnomalies.dist_matrix","page":"Distance and Densities","title":"MultivariateAnomalies.dist_matrix","text":"dist_matrix(data::AbstractArray{tp, N}; dist::String = \"Euclidean\", space::Int = 0, lat::Int = 0, lon::Int = 0, Q = 0) where {tp, N}\ndist_matrix(data::AbstractArray{tp, N}, training_data; dist::String = \"Euclidean\", space::Int = 0, lat::Int = 0, lon::Int = 0, Q = 0) where {tp, N}\n\ncompute the distance matrix of data or the distance matrix between data and training data i.e. the pairwise distances along the first dimension of data, using the last dimension as variables. dist is a distance metric, currently Euclidean(default), SqEuclidean, Chebyshev, Cityblock, JSDivergence, Mahalanobis and SqMahalanobis are supported. The latter two need a covariance matrix Q as input argument.\n\nExamples\n\njulia> dc = randn(10, 4,3)\njulia> D = dist_matrix(dc, space = 2)\n\n\n\n\n\n","category":"function"},{"location":"man/DistancesDensity/#MultivariateAnomalies.dist_matrix!","page":"Distance and Densities","title":"MultivariateAnomalies.dist_matrix!","text":"dist_matrix!(D_out, data, ...)\n\ncompute the distance matrix of data, similar to dist_matrix(). D_out object has to be preallocated, i.e. with init_dist_matrix.\n\nExamples\n\njulia> dc = randn(10,4, 4,3)\njulia> D_out = init_dist_matrix(dc)\njulia> dist_matrix!(D_out, dc, lat = 2, lon = 2)\njulia> D_out[1]\n\n\n\n\n\n","category":"function"},{"location":"man/DistancesDensity/#MultivariateAnomalies.init_dist_matrix","page":"Distance and Densities","title":"MultivariateAnomalies.init_dist_matrix","text":"init_dist_matrix(data)\ninit_dist_matrix(data, training_data)\n\ninitialize a D_out object for dist_matrix!().\n\n\n\n\n\n","category":"function"},{"location":"man/DistancesDensity/#k-Nearest-Neighbor-Objects-1","page":"Distance and Densities","title":"k-Nearest Neighbor Objects","text":"","category":"section"},{"location":"man/DistancesDensity/#","page":"Distance and Densities","title":"Distance and Densities","text":"k-Nearest Neighbor objects return the k nearest points and their distance out of a distance matrix D.","category":"page"},{"location":"man/DistancesDensity/#Functions-2","page":"Distance and Densities","title":"Functions","text":"","category":"section"},{"location":"man/DistancesDensity/#","page":"Distance and Densities","title":"Distance and Densities","text":"knn_dists\nknn_dists!\ninit_knn_dists","category":"page"},{"location":"man/DistancesDensity/#MultivariateAnomalies.knn_dists","page":"Distance and Densities","title":"MultivariateAnomalies.knn_dists","text":"knn_dists(D, k::Int, temp_excl::Int = 0)\n\nreturns the k-nearest neighbors of a distance matrix D. Excludes temp_excl (default: temp_excl = 0) distances from the main diagonal of D to be also nearest neighbors.\n\nExamples\n\njulia> dc = randn(20, 4,3)\njulia> D = dist_matrix(dc, space = 2)\njulia> knn_dists_out = knn_dists(D, 3, 1)\njulia> knn_dists_out[5] # distances\njulia> knn_dists_out[4] # indices\n\n\n\n\n\n","category":"function"},{"location":"man/DistancesDensity/#MultivariateAnomalies.knn_dists!","page":"Distance and Densities","title":"MultivariateAnomalies.knn_dists!","text":"knn_dists!(knn_dists_out, D, temp_excl::Int = 0)\n\nreturns the k-nearest neighbors of a distance matrix D. Similar to knn_dists(), but uses preallocated input object knn_dists_out, initialized with init_knn_dists(). Please note that the number of nearest neighbors k is not necessary, as it is already determined by the knn_dists_out object.\n\nExamples\n\njulia> dc = randn(20, 4,3)\njulia> D = dist_matrix(dc, space = 2)\njulia> knn_dists_out = init_knn_dists(dc, 3)\njulia> knn_dists!(knn_dists_out, D)\njulia> knn_dists_out[5] # distances\njulia> knn_dists_out[4] # indices\n\n\n\n\n\n","category":"function"},{"location":"man/DistancesDensity/#MultivariateAnomalies.init_knn_dists","page":"Distance and Densities","title":"MultivariateAnomalies.init_knn_dists","text":"init_knn_dists(T::Int, k::Int)\ninit_knn_dists(datacube::AbstractArray, k::Int)\n\ninitialize a preallocated knn_dists_out object. kis the number of nerarest neighbors, T the number of time steps (i.e. size of the first dimension) or a multidimensional datacube.\n\n\n\n\n\n","category":"function"},{"location":"man/DistancesDensity/#Kernel-Matrices-(Dissimilarities)-1","page":"Distance and Densities","title":"Kernel Matrices (Dissimilarities)","text":"","category":"section"},{"location":"man/DistancesDensity/#","page":"Distance and Densities","title":"Distance and Densities","text":"A distance matrix D can be converted into a kernel matrix K, i.e. by computing pairwise dissimilarities using Gaussian kernels centered on each datapoint. ","category":"page"},{"location":"man/DistancesDensity/#","page":"Distance and Densities","title":"Distance and Densities","text":"K= exp(-05 cdot D cdot sigma^-2)","category":"page"},{"location":"man/DistancesDensity/#Functions-3","page":"Distance and Densities","title":"Functions","text":"","category":"section"},{"location":"man/DistancesDensity/#","page":"Distance and Densities","title":"Distance and Densities","text":"kernel_matrix\nkernel_matrix!","category":"page"},{"location":"man/DistancesDensity/#MultivariateAnomalies.kernel_matrix","page":"Distance and Densities","title":"MultivariateAnomalies.kernel_matrix","text":"kernel_matrix(D::AbstractArray, σ::Float64 = 1.0[, kernel::String = \"gauss\", dimension::Int64 = 1])\n\ncompute a kernel matrix out of distance matrix D, given σ. Optionally normalized by the dimension, if kernel = \"normalized_gauss\". compute D with dist_matrix().\n\nExamples\n\njulia> dc = randn(20, 4,3)\njulia> D = dist_matrix(dc, space = 2)\njulia> K = kernel_matrix(D, 2.0)\n\n\n\n\n\n","category":"function"},{"location":"man/DistancesDensity/#MultivariateAnomalies.kernel_matrix!","page":"Distance and Densities","title":"MultivariateAnomalies.kernel_matrix!","text":"kernel_matrix!(K, D::AbstractArray, σ::Float64 = 1.0[, kernel::String = \"gauss\", dimension::Int64 = 1])\n\ncompute a kernel matrix out of distance matrix D. Similar to kernel_matrix(), but with preallocated Array K (K = similar(D)) for output.\n\nExamples\n\njulia> dc = randn(20, 4,3)\njulia> D = dist_matrix(dc, space = 2)\njulia> kernel_matrix!(D, D, 2.0) # overwrites distance matrix\n\n\n\n\n\n","category":"function"},{"location":"man/DistancesDensity/#Index-1","page":"Distance and Densities","title":"Index","text":"","category":"section"},{"location":"man/DistancesDensity/#","page":"Distance and Densities","title":"Distance and Densities","text":"Pages = [\"DistancesDensity.md\"]","category":"page"},{"location":"man/Postprocessing/#Scores-1","page":"Postprocessing","title":"Scores","text":"","category":"section"},{"location":"man/Postprocessing/#","page":"Postprocessing","title":"Postprocessing","text":"Postprocess your anomaly scores by making different algorithms comparable and computing their ensemble.","category":"page"},{"location":"man/Postprocessing/#Functions-1","page":"Postprocessing","title":"Functions","text":"","category":"section"},{"location":"man/Postprocessing/#","page":"Postprocessing","title":"Postprocessing","text":"get_quantile_scores\nget_quantile_scores!\ncompute_ensemble","category":"page"},{"location":"man/Postprocessing/#MultivariateAnomalies.get_quantile_scores","page":"Postprocessing","title":"MultivariateAnomalies.get_quantile_scores","text":"get_quantile_scores(scores, quantiles = 0.0:0.01:1.0)\n\nreturn the quantiles of the given N dimensional anomaly scores cube. quantiles (default: quantiles = 0.0:0.01:1.0) is a Float range of quantiles. Any score being greater or equal quantiles[i] and beeing smaller than quantiles[i+1] is assigned to the respective quantile quantiles[i].\n\nExamples\n\njulia> scores1 = rand(10, 2)\njulia> quantile_scores1 = get_quantile_scores(scores1)\n\n\n\n\n\n","category":"function"},{"location":"man/Postprocessing/#MultivariateAnomalies.get_quantile_scores!","page":"Postprocessing","title":"MultivariateAnomalies.get_quantile_scores!","text":"get_quantile_scores!{tp,N}(quantile_scores::AbstractArray{Float64, N}, scores::AbstractArray{tp,N}, quantiles::StepRangeLen{Float64} = 0.0:0.01:1.0)\n\nreturn the quantiles of the given N dimensional scores array into a preallocated quantile_scores array, see get_quantile_scores().\n\n\n\n\n\n","category":"function"},{"location":"man/Postprocessing/#MultivariateAnomalies.compute_ensemble","page":"Postprocessing","title":"MultivariateAnomalies.compute_ensemble","text":"compute_ensemble(m1_scores, m2_scores[, m3_scores, m4_scores]; ensemble = \"mean\")\n\ncompute the mean (ensemble = \"mean\"), minimum (ensemble = \"min\"), maximum (ensemble = \"max\") or median (ensemble = \"median\") of the given anomaly scores. Supports between 2 and 4 scores input arrays (m1_scores, ..., m4_scores). The scores of the different anomaly detection algorithms should be somehow comparable, e.g., by using get_quantile_scores() before.\n\nExamples\n\njulia> using MultivariateAnomalies\njulia> scores1 = rand(10, 2)\njulia> scores2 = rand(10, 2)\njulia> quantile_scores1 = get_quantile_scores(scores1)\njulia> quantile_scores2 = get_quantile_scores(scores2)\njulia> compute_ensemble(quantile_scores1, quantile_scores2, ensemble = \"max\")\n\n\n\n\n\n","category":"function"},{"location":"man/Postprocessing/#Index-1","page":"Postprocessing","title":"Index","text":"","category":"section"},{"location":"man/Postprocessing/#","page":"Postprocessing","title":"Postprocessing","text":"Pages = [\"Postprocessing.md\"]","category":"page"},{"location":"man/Preprocessing/#Feature-Extraction-Techniques-1","page":"Preprocessing","title":"Feature Extraction Techniques","text":"","category":"section"},{"location":"man/Preprocessing/#","page":"Preprocessing","title":"Preprocessing","text":"Extract the relevant inforamtion out of your data and use them as input feature for the anomaly detection algorithms.","category":"page"},{"location":"man/Preprocessing/#Dimensionality-Reduction-1","page":"Preprocessing","title":"Dimensionality Reduction","text":"","category":"section"},{"location":"man/Preprocessing/#","page":"Preprocessing","title":"Preprocessing","text":"For dimensionality reduction, we would like to point to the package MultivariateStats.jl. Several techniques are implemented there, e.g.","category":"page"},{"location":"man/Preprocessing/#","page":"Preprocessing","title":"Preprocessing","text":"Principal Component Analysis (PCA)\nIndependent Component Analysis (ICA)","category":"page"},{"location":"man/Preprocessing/#Seasonality-1","page":"Preprocessing","title":"Seasonality","text":"","category":"section"},{"location":"man/Preprocessing/#","page":"Preprocessing","title":"Preprocessing","text":"When dealing with time series, i.e. the observations are time steps, it might be important to remove or get robust estimates of the mean seasonal cycles. This is implemended by","category":"page"},{"location":"man/Preprocessing/#","page":"Preprocessing","title":"Preprocessing","text":"subtracting the median seasonal cycle (sMSC) and\ngetting the median seasonal cycle (get_MedianCycles)","category":"page"},{"location":"man/Preprocessing/#Functions-1","page":"Preprocessing","title":"Functions","text":"","category":"section"},{"location":"man/Preprocessing/#","page":"Preprocessing","title":"Preprocessing","text":"sMSC\nget_MedianCycles\nget_MedianCycle\nget_MedianCycle!\ninit_MedianCycle","category":"page"},{"location":"man/Preprocessing/#MultivariateAnomalies.sMSC","page":"Preprocessing","title":"MultivariateAnomalies.sMSC","text":"sMSC(datacube, cycle_length)\n\nsubtract the median seasonal cycle from the datacube given the length of year cycle_length.\n\nExamples\n\njulia> dc = hcat(rand(193) + 2* sin.(0:pi/24:8*pi), rand(193) + 2* sin.(0:pi/24:8*pi))\njulia> sMSC_dc = sMSC(dc, 48)\n\n\n\n\n\n","category":"function"},{"location":"man/Preprocessing/#MultivariateAnomalies.get_MedianCycles","page":"Preprocessing","title":"MultivariateAnomalies.get_MedianCycles","text":"get_MedianCycles(datacube, cycle_length::Int = 46)\n\nreturns the median annual cycle of a datacube, given the length of the annual cycle (presetting: cycle_length = 46). The datacube can be 2, 3, 4-dimensional, time is stored along the first dimension.\n\nExamples\n\njulia> using MultivariateAnomalies\njulia> dc = hcat(rand(193) + 2* sin(0:pi/24:8*pi), rand(193) + 2* sin(0:pi/24:8*pi))\njulia> cycles = get_MedianCycles(dc, 48)\n\n\n\n\n\n","category":"function"},{"location":"man/Preprocessing/#MultivariateAnomalies.get_MedianCycle","page":"Preprocessing","title":"MultivariateAnomalies.get_MedianCycle","text":"get_MedianCycle(dat::Array{tp,1}, cycle_length::Int = 46)\n\nreturns the median annual cycle of a one dimensional data array, given the length of the annual cycle (presetting: cycle_length = 46). Can deal with some NaN values.\n\nExamples\n\njulia> using MultivariateAnomalies\njulia> dat = randn(90) + x = sind.(0:8:719)\njulia> cycles = get_MedianCycle(dat, 48)\n\n\n\n\n\n","category":"function"},{"location":"man/Preprocessing/#MultivariateAnomalies.get_MedianCycle!","page":"Preprocessing","title":"MultivariateAnomalies.get_MedianCycle!","text":"get_MedianCycle!(init_MC, dat::Array{tp,1})\n\nMemory efficient version of get_MedianCycle(), returning the median cycle in init_MC[3]. The init_MC object should be created with init_MedianCycle. Can deal with some NaN values.\n\nExamples\n\njulia> using MultivariateAnomalies\njulia> dat = rand(193) + 2* sin(0:pi/24:8*pi)\njulia> dat[100] = NaN\njulia> init_MC = init_MedianCycle(dat, 48)\njulia> get_MedianCycle!(init_MC, dat)\njulia> init_MC[3]\n\n\n\n\n\n","category":"function"},{"location":"man/Preprocessing/#MultivariateAnomalies.init_MedianCycle","page":"Preprocessing","title":"MultivariateAnomalies.init_MedianCycle","text":"init_MedianCycle(dat::Array{tp}, cycle_length::Int = 46)\ninit_MedianCycle(temporal_length::Int[, cycle_length::Int = 46])\n\ninitialises an initMC object to be used as input for `getMedianCycle!(). Input is either some sample data or the temporal lenght of the expected input vector and the length of the annual cycle (presetting:cycle_length = 46`)\n\n\n\n\n\n","category":"function"},{"location":"man/Preprocessing/#Exponential-Weighted-Moving-Average-1","page":"Preprocessing","title":"Exponential Weighted Moving Average","text":"","category":"section"},{"location":"man/Preprocessing/#","page":"Preprocessing","title":"Preprocessing","text":"One option to reduce the noise level in the data and detect more 'significant' anomalies is computing an exponential weighted moving average (EWMA)","category":"page"},{"location":"man/Preprocessing/#Function-1","page":"Preprocessing","title":"Function","text":"","category":"section"},{"location":"man/Preprocessing/#","page":"Preprocessing","title":"Preprocessing","text":"EWMA\nEWMA!","category":"page"},{"location":"man/Preprocessing/#MultivariateAnomalies.EWMA","page":"Preprocessing","title":"MultivariateAnomalies.EWMA","text":"EWMA(dat,  λ)\n\nCompute the exponential weighted moving average (EWMA) with the weighting parameter λ between 0 (full weighting) and 1 (no weighting) along the first dimension of dat. Supports N-dimensional Arrays.\n\nLowry, C. A., & Woodall, W. H. (1992). A Multivariate Exponentially Weighted Moving Average Control Chart. Technometrics, 34, 46–53.\n\nExamples\n\njulia> dc = rand(100,3,2)\njulia> ewma_dc = EWMA(dc, 0.1)\n\n\n\n\n\n","category":"function"},{"location":"man/Preprocessing/#MultivariateAnomalies.EWMA!","page":"Preprocessing","title":"MultivariateAnomalies.EWMA!","text":"EWMA!(Z, dat,  λ)\n\nuse a preallocated output Z. Z = similar(dat) or dat = dat for overwriting itself.\n\nExamples\n\njulia> dc = rand(100,3,2)\njulia> EWMA!(dc, dc, 0.1)\n\n\n\n\n\n","category":"function"},{"location":"man/Preprocessing/#Time-Delay-Embedding-1","page":"Preprocessing","title":"Time Delay Embedding","text":"","category":"section"},{"location":"man/Preprocessing/#","page":"Preprocessing","title":"Preprocessing","text":"Increase the feature space (Variabales) with lagged observations. ","category":"page"},{"location":"man/Preprocessing/#Function-2","page":"Preprocessing","title":"Function","text":"","category":"section"},{"location":"man/Preprocessing/#","page":"Preprocessing","title":"Preprocessing","text":"TDE","category":"page"},{"location":"man/Preprocessing/#MultivariateAnomalies.TDE","page":"Preprocessing","title":"MultivariateAnomalies.TDE","text":"TDE(datacube::Array{tp, 4}, ΔT::Integer, DIM::Int = 3) where {tp}\nTDE(datacube::Array{tp, 3}, ΔT::Integer, DIM::Int = 3) where {tp}\n\nreturns an embedded datacube by concatenating lagged versions of the 2-, 3- or 4-dimensional datacube with ΔT time steps in the past up to dimension DIM (presetting: DIM = 3)\n\nExamples\n\njulia> dc = randn(50,3)\njulia> TDE(dc, 3, 2)\n\n\n\n\n\n","category":"function"},{"location":"man/Preprocessing/#Moving-Window-Features-1","page":"Preprocessing","title":"Moving Window Features","text":"","category":"section"},{"location":"man/Preprocessing/#","page":"Preprocessing","title":"Preprocessing","text":"include the variance (mwVAR) and correlations (mwCOR) in a moving window along the first dimension of the data.","category":"page"},{"location":"man/Preprocessing/#Functions-2","page":"Preprocessing","title":"Functions","text":"","category":"section"},{"location":"man/Preprocessing/#","page":"Preprocessing","title":"Preprocessing","text":"mw_VAR\nmw_VAR!\nmw_COR\nmw_AVG\nmw_AVG!","category":"page"},{"location":"man/Preprocessing/#MultivariateAnomalies.mw_VAR","page":"Preprocessing","title":"MultivariateAnomalies.mw_VAR","text":"mw_VAR(datacube::Array{tp,N}, windowsize::Int = 10) where {tp,N}\n\ncompute the variance in a moving window along the first dimension of the datacube (presetting: windowsize = 10). Accepts N dimensional datacubes.\n\nExamples\n\njulia> dc = randn(50,3,3,3)\njulia> mw_VAR(dc, 15)\n\n\n\n\n\n","category":"function"},{"location":"man/Preprocessing/#MultivariateAnomalies.mw_VAR!","page":"Preprocessing","title":"MultivariateAnomalies.mw_VAR!","text":"mw_VAR!(out::Array{tp, N}, datacube0mean::Array{tp,N}, windowsize::Int = 10) where {tp,N}\n\nmutating version for mw_VAR(). The mean of the input data datacube0mean has to be 0. Initialize out properly: out = datacube0mean leads to wrong results.\n\n\n\n\n\n","category":"function"},{"location":"man/Preprocessing/#MultivariateAnomalies.mw_COR","page":"Preprocessing","title":"MultivariateAnomalies.mw_COR","text":"mw_COR(datacube::Array{tp, 4}, windowsize::Int = 10) where {tp}\n\ncompute the correlation in a moving window along the first dimension of the datacube (presetting: windowsize = 10). Accepts 4-dimensional datacubes.\n\n\n\n\n\n","category":"function"},{"location":"man/Preprocessing/#MultivariateAnomalies.mw_AVG","page":"Preprocessing","title":"MultivariateAnomalies.mw_AVG","text":"mw_AVG(datacube::AbstractArray{tp,N}, windowsize::Int = 10) where {tp,N}\n\ncompute the average in a moving window along the first dimension of the datacube (presetting: windowsize = 10). Accepts N dimensional datacubes.\n\nExamples\n\njulia> dc = randn(50,3,3,3)\njulia> mw_AVG(dc, 15)\n\n\n\n\n\n","category":"function"},{"location":"man/Preprocessing/#MultivariateAnomalies.mw_AVG!","page":"Preprocessing","title":"MultivariateAnomalies.mw_AVG!","text":"mw_AVG!(out::Array{tp, N}, datacube::Array{tp,N}, windowsize::Int = 10) where {tp,N}\n\ninternal and mutating version for mw_AVG().\n\n\n\n\n\n","category":"function"},{"location":"man/Preprocessing/#Index-1","page":"Preprocessing","title":"Index","text":"","category":"section"},{"location":"man/Preprocessing/#","page":"Preprocessing","title":"Preprocessing","text":"Pages = [\"Preprocessing.md\"]","category":"page"},{"location":"man/AUC/#Area-Under-the-Curve-1","page":"AUC","title":"Area Under the Curve","text":"","category":"section"},{"location":"man/AUC/#","page":"AUC","title":"AUC","text":"Compute true positive rates, false positive rates and the area under the curve to evaulate the algorihtms performance. Efficient implementation according to","category":"page"},{"location":"man/AUC/#","page":"AUC","title":"AUC","text":"Fawcett, T. (2006). An introduction to ROC analysis. Pattern Recognition Letters, 27(8), 861–874. Link","category":"page"},{"location":"man/AUC/#Functions-1","page":"AUC","title":"Functions","text":"","category":"section"},{"location":"man/AUC/#","page":"AUC","title":"AUC","text":"auc","category":"page"},{"location":"man/AUC/#MultivariateAnomalies.auc","page":"AUC","title":"MultivariateAnomalies.auc","text":"auc(scores, events, increasing = true)\n\ncompute the Area Under the receiver operator Curve (AUC), given some output scores array and some ground truth (events). By default, it is assumed, that the scores are ordered increasingly (increasing = true), i.e. high scores represent events. AUC is computed according to Fawcett, T. (2006). An introduction to ROC analysis. Pattern Recognition Letters, 27(8), 861–874. http://doi.org/10.1016/j.patrec.2005.10.010\n\nExamples\n\njulia> scores = rand(10, 2)\njulia> events = rand(0:1, 10, 2)\njulia> auc(scores, events)\njulia> auc(scores, boolevents(events))\n\n\n\n\n\n","category":"function"},{"location":"man/AUC/#","page":"AUC","title":"AUC","text":"Index","category":"page"},{"location":"man/AUC/#","page":"AUC","title":"AUC","text":"Pages = [\"AUC.md\"]","category":"page"},{"location":"man/OnlineAlgorithms/#Online-Algorithms-1","page":"OnlineAlgorithms","title":"Online Algorithms","text":"","category":"section"},{"location":"man/OnlineAlgorithms/#","page":"OnlineAlgorithms","title":"OnlineAlgorithms","text":"We provide online some functions, which are tuned to allocate minimal amounts of memory. Implemented so far: ","category":"page"},{"location":"man/OnlineAlgorithms/#","page":"OnlineAlgorithms","title":"OnlineAlgorithms","text":"Euclidean distance\nSigma estimation for KDE / REC\nKDE\nREC\nKNN-Gamma ","category":"page"},{"location":"man/OnlineAlgorithms/#Functions-1","page":"OnlineAlgorithms","title":"Functions","text":"","category":"section"},{"location":"man/OnlineAlgorithms/#","page":"OnlineAlgorithms","title":"OnlineAlgorithms","text":"Euclidean_distance!\nSigmaOnline!\nKDEonline!\nREConline!\nKNNonline!","category":"page"},{"location":"man/OnlineAlgorithms/#MultivariateAnomalies.Euclidean_distance!","page":"OnlineAlgorithms","title":"MultivariateAnomalies.Euclidean_distance!","text":"Euclidean_distance!{tp}(d::Array{tp, 1}, x::AbstractArray{tp, 2}, i::Int, j::Int, dim::Int = 1)\n\ncompute the Euclidean distance between x[i,:] and x[j,:] and write the result to d. Memory efficient. dim is the dimension of i and j.\n\n\n\n\n\n","category":"function"},{"location":"man/OnlineAlgorithms/#MultivariateAnomalies.SigmaOnline!","page":"OnlineAlgorithms","title":"MultivariateAnomalies.SigmaOnline!","text":"SigmaOnline!{tp}(sigma::Array{tp, 1}, x::AbstractArray{tp, 2}, [Q::AbstractArray{tp, 2}], samplesize::Int = 250, dim::Int = 1)\n\ncompute sigma parameter as mean of the distances of samplesize randomly sampled points along dim. If Q is given the Mahalanobis distance is used instead of Euclidean.\n\n\n\n\n\n","category":"function"},{"location":"man/OnlineAlgorithms/#MultivariateAnomalies.KDEonline!","page":"OnlineAlgorithms","title":"MultivariateAnomalies.KDEonline!","text":"KDEonline!{tp}(kdescores::AbstractArray{tp, 1}, x::AbstractArray{tp, 2} [, Q::AbstractArray{tp, 2}], σ::tp, dim::Int = 1)\n\ncompute (1.0 - Kernel Density Estimates) from x and write it to kdescores with dim being the dimension of the observations. If teh covariance matrix Q is given, the Mahalanobis distance is used instead of the Euclidean distance.\n\n\n\n\n\n","category":"function"},{"location":"man/OnlineAlgorithms/#MultivariateAnomalies.REConline!","page":"OnlineAlgorithms","title":"MultivariateAnomalies.REConline!","text":"REConline!{tp}(recscores::AbstractArray{tp, 1}, x::AbstractArray{tp, 2} [, Q::AbstractArray{tp, 2}], ɛ::tp, dim::Int = 1)\n\ncompute recurrence scores from x and write it to recscores with dim being the dimension of the observations. If the covariance matrix Q is given, the mahalanobis distance is used instead of the euclidean distance.\n\n\n\n\n\n","category":"function"},{"location":"man/OnlineAlgorithms/#MultivariateAnomalies.KNNonline!","page":"OnlineAlgorithms","title":"MultivariateAnomalies.KNNonline!","text":"KNNonline!{tp}(knnscores::AbstractArray{tp, 1}, x::AbstractArray{tp, 2}, [Q::AbstractArray{tp, 2},] k::Int, dim::Int = 1)\n\ncompute k-nearest neighbor (gamma) scores from x and write it to knnscores with dim being the dimension of the observations. If the covariance matrix Q is given, the mahalanobis distance is used instead of the euclidean distance.\n\n\n\n\n\n","category":"function"},{"location":"man/OnlineAlgorithms/#Index-1","page":"OnlineAlgorithms","title":"Index","text":"","category":"section"},{"location":"man/OnlineAlgorithms/#","page":"OnlineAlgorithms","title":"OnlineAlgorithms","text":"Pages = [\"OnlineAlgorithms.md\"]","category":"page"}]
}
